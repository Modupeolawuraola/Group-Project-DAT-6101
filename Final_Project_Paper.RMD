---
title: "Predicting Bike Sharing Demand in the Cities"
author: "Modupeola Fagbenro, Crystal Kao, Guruksha Gurnani, Shanun Randev"
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---
# Abstract

In recent years, bike-sharing systems have gained widespread popularity as a sustainable and convenient mode of transportation in urban areas. This study explores a dataset from the Capital Bikeshare program in Washington, D.C., spanning 2011-2012, comprising 10,000 rows and 15 columns. The dataset includes key variables such as season, holiday status, working day indicator, weather conditions, temperature, "feels-like" temperature, humidity, windspeed, and the counts of casual, registered, and total bike rentals at different hours.
The research aims to investigate factors influencing bike-sharing patterns, focusing on urban rentals, weather conditions, and the overall growth in bike rentals over the years. SMART questions guide the analysis, addressing the relevance of various variables in predicting bike usage and exploring the impact of temperature on rental patterns. Additionally, the study delves into understanding the temporal trends in casual bike rentals.
Through rigorous analysis, this research aims to contribute insights into the dynamics of bike-sharing systems and their sensitivity to environmental and temporal factors. The findings could inform urban planning and policy decisions promoting sustainable transportation alternatives.  

# Introduction 
## Background of the study
The given data set encompasses information about bike sharing, a rapidly growing sustainable mode of transportation. Bike-sharing systems have been gaining more popularity in urban areas than in rural ones due to the convenience and ease of traveling. This eco-friendly system allows individuals to rent bikes for short periods typically on an hourly basis, making them an efficient choice of commute, and leisure and reducing environmental impact.
Our data spans over 10,000 rows across 15 columns– season, Holiday, working day, Weather, Temperature, Atemp, Humidity, windspeed, casual, registered, count, datetime.

As a direct quote from the data source: 

  ``Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.

The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city…" (Cukierski, 2014)

The data is specifically collected from the Capital Bikeshare program in Washington, D.C for 2011-2012.


## Literature Review
There have been three generations of bike-sharing systems over the past 45 years (DeMaio, 2004). The 1st generation of bike-sharing programs began on July 28, 1965, in Amsterdam with the Witte Fietsen, or White Bikes (Schimmelpennick, 2009). Ordinary bikes, painted white, were provided for public use. One could find a bike, ride it to his or her destination, and leave it for the next user. Things did not go as planned, as bikes were thrown into the canals or appropriated for private use. The program collapsed within days.
In 1991, the inception of the 2nd generation of bike-sharing programs occurred in Farsø and Grenå, Denmark, followed by Nakskov, Denmark, in 1993 (Nielse, 1993). These initiatives were relatively small in scale, with Nakskov featuring 26 bikes distributed across 4 stations. It wasn't until 1995 that the initial large-scale 2nd generation bike-sharing program emerged in Copenhagen, known as Bycyklen or City Bikes. This program marked substantial improvements over its predecessor. The Copenhagen bikes were meticulously crafted for robust utilitarian usage, equipped with solid rubber tires, wheels adorned with advertising plates, and a system enabling users to pick up and return bikes at designated locations throughout the central city with a coin deposit. Although more structured than the previous generation, incorporating stations and a non-profit organization for operation, these bikes still faced theft challenges due to user anonymity. This circumstance led to the development of a new generation of bike-sharing, featuring enhanced customer tracking.
The initiation of the latest iteration of 3rd generation bike-sharing programs commenced with Bikeabout in 1996 at Portsmouth University in England. At this institution, students were afforded the convenience of using a magnetic stripe card for bike rentals (Black and Potter, n.d.). This marked the onset of an evolving era for 3rd generation bike-sharing systems, characterized by a range of technological enhancements. These improvements encompassed features like electronically-locking racks or bike locks, telecommunication systems, smart cards and fobs, mobile phone access, and on-board computers.
The subsequent years witnessed a gradual expansion of bike-sharing, with the introduction of one or two new programs annually. Examples include Rennes’ (France) Vélo à la Carte in 1998 and Munich’s Call a Bike in 2000. However, the breakthrough for 3rd generation bike-sharing occurred in 2005 with the launch of Velo’v, featuring 1,500 bikes in Lyon by JCDecaux (Optimising Bike Sharing in European Cities, 2009). This program stood out as the largest 3rd generation bike-sharing initiative to date, and its impact was evident. By late 2005, Lyon's Velo’v boasted 15,000 members, and the bikes were being used an average of 6.5 times each day. This success caught the attention of Paris, Lyon's significant counterpart (Henley, 2005).
Two years later, Paris unveiled its bike-sharing initiative, Vélib', featuring approximately 7,000 bikes. This program has since undergone substantial expansion, with the current count reaching 23,600 bikes across the city and suburbs. The magnitude of this ambitious endeavor, coupled with its surpassing success, significantly altered the trajectory of bike-sharing history, garnering widespread interest in this mode of transit on a global scale. Beyond Europe, the year 2008 marked the inception of bike-sharing programs in Brazil, Chile, China, New Zealand, South Korea, Taiwan, and the U.S. Each of these programs represented the inaugural 3rd generation bike-sharing initiative for its respective country.
By the close of 2007, the global count of 3rd generation bike-sharing programs stood at approximately 60 (DeMaio, 2007). This number experienced further growth, reaching about 92 programs by the end of 2008 (DeMaio, 2008). Presently, there are approximately 120 programs. Existing 3rd generation programs are denoted with a cyclist icon, while planned programs are indicated with a question mark icon (MetroBike, 2009).


## Problem Statement
In bustling urban centers characterized by heavy traffic and relatively short distances between points of interest, bike sharing has emerged as a viable alternative mode of transportation. Bike sharing allows individuals to conveniently rent bicycles from designated stations, without the requirement of returning them to the same station. Instead, there are numerous drop-off locations scattered throughout the city, offering increased flexibility in returning bicycles.

## Research Aim and Objectives 
The objective of this research is to check factors that can influence Bike Sharing:

1.	Checking Number of Urban Rentals Higher casual or registered population density in urban areas typically makes bike sharing more appealing due to shorter distances between destinations and increased demand for alternative transportation options.
2.	Weather Conditions: Weather, such as temperature, humidity, or extreme heat, can impact bike sharing. Cities with more favorable weather conditions may see higher bike-sharing usage.
3.	High-year Rentals: The increase in bikes year in and year out can be a result of good weather conditions or an increase in awareness of alternative means of transportation such as bike sharing.

## Research SMART Questions
Specific Measurable Achievable Relevant and Time-Bound Questions (SMART):
Here are the SMART Questions, our analysis aims to answer:

1.	Which variables are useful for predicting the number of bikes used in an hour?
2.	How does temperature affect the number of bikes in use?
3.	When do casual bikers increase the most?
4.  Can we estimate when it is a holiday depending on other bike share factors?

Applying the SMART framework to our questions allows us to set clear objectives, measure progress, and ensure that the analysis provides meaningful insight for decision-makers/stakeholders in the context of bike sharing.

## Dataset Fields

-	Datetime – The hour, date, and timestamp of each observation.
-	Season: Categorical variable indicating the season, which can affect bike usage (e.g., 1-spring, 2-summer, 3-fall, 4-winter).
-	Holiday: It’s binary variable indicating whether it's a holiday (1 for yes, 0 for no)
-	Working Day: It's a binary variable indicating whether it's a working day (1 for yes, 0 for no).
+	Weather: Categorical variable representing different weather conditions, influencing bike ridership (e.g., clear, cloudy, rainy).
    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
-	Temperature (temp): Temperature in Celsius, affecting user preferences for bike rides.
-	Atemp : "Feels Like" Temperature (atemp): This column represents the "feels like" temperature, which takes into account factors such as wind chill and humidity to provide a more accurate perception of temperature. It can affect user comfort and, consequently, bike usage.
-	Humidity: Humidity level, which can impact the comfort of cycling.
-	Windspeed: Windspeed, another weather-related factor that affects bike riding conditions.
-	Casual: The number of casual (non-registered) bike rentals.
-	Registered: The number of registered bike rentals, indicating regular users of the bike sharing system.
-	Count: The total count of bike rentals, which is the sum of casual and registered rentals, serving as the primary target variable for analysis and prediction.

Based on the above information, this data set allows us to explore how different factors including but not limited to weather conditions, time of the day, and humidity can impact bike-sharing trends and use these insights to improve bike-sharing systems.

## Dataset Limitations

1.	Our Above data-set has limited variables and features, which has limitations, we are unable to explore the Pricing and Accessibility of bikes: The cost of bike-sharing services and easy the accessibility of bike stations can be significant factors in attracting users. Affordable pricing and convenient station locations encourage usage.
2.	The data set does not include Bike Promotion and Marketing: Effective marketing, awareness, and promotion campaigns can boost bike-sharing programs' visibility and encourage the participation of more people
3.	The data-set did not explicitly have a Technological Integration features example The use of smartphone apps for finding bike locations and unlocking bikes, as well as using smartphones in tracking services, can make bike sharing more convenient and appealing to users and can lead to increase number of people willing to use bike sharing as an alternative way of transportation.
4.	The data-set does not show Economic Factors features, the economic situation of the city's residents, including income levels and employment opportunities, can influence the willingness to use bike-sharing services.
5.	The data-set does not include the Competition level among bike sharing, The presence of other transportation options, such as public transit, taxis, or ride-sharing services, can affect bike-sharing adoption rates.

# Exploratory Data Analysis and Methodology
Performing exploratory data analysis (EDA) on a bike-sharing dataset is an essential step to gain insights and understand the data before diving into more advanced analyses. Here's a step-by-step guide to performing EDA on a bike-sharing dataset and what methods we used:

## EDA Steps Conducted:

1.	**Data Collection**: Start by obtaining the bike-sharing dataset from Kaggle. https://www.kaggle.com/competitions/bike-sharing-demand/data
2.	**Data Loading**: Import the dataset into R studio data analysis environment. 

```{r, warning=FALSE} 
#Reading the CSV file into data frame
train_data <- read.csv('train.csv', stringsAsFactors = FALSE)
```

3.	**Load Useful Packages**:  The packages used in this analysis include: tidyverse, psych, ggthemes, ggplot2, dplyr, tidyr, corrgram, lubridate, caret, corrplot.
```{r, setup= FALSE, warning=FALSE} 
#Loading libraries
library(tidyverse)
library(psych)
library(ggthemes)
library(ggplot2) #for data visualization
library(dplyr) #for data manipulation
library(tidyr) #for tidying the data
library(corrgram)
library(lubridate)
library(caret)
library(corrplot)
library(leaps)
library(bestglm)
```

4.	**Data Overview**: Check the first few rows of the dataset to understand its structure and the kind of information it contains. Next check the data types of each column (e.g., categorical datatypes and numerical datatypes) to ensure they are correctly interpreted. Finally, inspect the columns (variables) in the dataset and their names.
```{r, setup= FALSE, warning=FALSE} 
 
## Getting Data Frame dimensions
#displaying shape, number of columns, rows and first few rows of the data set
head(train_data)
nrow(train_data)  # Number of rows, 10866
ncol(train_data)  # Number of columns, 12

#Basic information about Data Frame
str(train_data)
```

```{r}
describe(train_data)
```

5.	**Data Cleaning**: Handle missing data by either imputing missing values or removing rows/columns with missing data. This data file luckily did not include any missing values or rows. 
6.	**Summary Statistics**: Calculate summary statistics like mean, median, standard deviation, and percentiles for numerical variables. This provides an initial understanding of the data distribution. For categorical variables, calculate frequency counts and proportions.
```{r summary}
summary(train_data) #summary statistics for the data frame
```

7.	**Data Visualization**: Create visualizations to gain insights into the data. Some plots we used include scatterplots, box plots, bar charts, and correlation matrices. 
```{r, echo=FALSE}
ggplot(train_data, aes(temp, count)) + geom_point(alpha = 0.3) + theme_bw() +
  labs(title = "Number of Bikers by Temperature") +
  xlab("Temperature (Celcius)") +
  ylab("Number of Bikers")
```

```{r, echo=FALSE}

ggplot(train_data, aes(humidity, count)) + geom_point(alpha = 0.3) + theme_bw() +
  labs(title = "Number of Bikers by Humidity") +
  xlab("Humidity (%)") +
  ylab("Number of Bikers")

```

```{r, echo=FALSE}
numeric_train_data <- train_data %>% select_if(is.numeric)
cor_matrix <- cor(numeric_train_data)

# Create the correlation plot
corrplot(cor_matrix, method = "color", type = "lower", tl.cex = 0.7)

```

```{r, echo=FALSE}

train_data$season <- as.factor(train_data$season)

# Create a box plot for 'cnt' by 'season'
ggplot(train_data, aes(x = season, y = count, fill = season, group = season)) +
  geom_boxplot() +
  labs(title = "Total # Bikes Rented By Season") +
  scale_fill_brewer(palette = "Set2") +
  xlab("Season") +
  ylab("Total # Bikes Rented")

```


8.	**Distributions and Patterns**: Examine the distribution of the target variable(s) (e.g., bike rentals) and potential predictor variables. For this analysis, we looked at bikers by temperature, bikers by humidity, and bikers by season, which will all be shown in the results section as they pertain to our SMART questions.

# Changes Made After EDA
After doing some EDA, some of our questions changed to adhere more to what made more sense for our data to answer and focus in on answering more pointed questions. For example, one of our original questions was which variable has the highest effect on the count of the number of bikes used in a day? However, due to the way the data is collected and the purpose of the dataset, it made more sense to change the question to which variables are useful for predicting the number of bikes used in an hour? Since the data is collected hourly and creating a model instead of looking for a specific variable made more sense. The model can be used to predict future bike sharing rates. 

# Model Selection and Hypothesis Testing
Since we were interested in learning about which variables influence the number of bikers in a given hour, we wanted to create a linear regression model. Before doing so, we needed to format our data appropriately. We created new fields based on the ‘Datetime’ column to split out the year, day, month, hour and day of the week. ‘Season’ and ‘weather’ are categorical variables and were represented using dummy variables.

```{r}
train_data$datetime <- as.POSIXct(train_data$datetime)


extract_feature <- function(df) {
  df <- df %>%
    mutate(
      year = year(datetime),
      day = day(datetime),
      month = month(datetime),
      hour = hour(datetime),
      dayofweek = wday(datetime) - 1  
    )
  
  return(df)
}


train_data$weather <- as.factor(train_data$weather)



train_data <- extract_feature(train_data)

```

We first started out by running a linear model on the data with ‘count’ as the dependent variable and all other available variables as the predictors (this is considered Model 1). However, we excluded ‘registered’ and ‘casual’ because they sum up to the ‘count’. 
```{r}

train_data <- train_data %>% select(-datetime)


training_df <- train_data[, !colnames(train_data) %in% c("casual","registered")]


lm_model <- lm(count ~ ., data = training_df)

print(summary(lm_model))

```

```{r}
par(mfrow = c(2,2))
plot(lm_model, which = c(1,2,3))
```

Based on the results of the residuals vs fitted plot, the variance of residuals increased as the values increased. 

A log transformation was considered to see if there was a model with more constant variance. Because of this, we also ran a version of the model with a log transformation on the ‘count’ field to see if the normality assumption would be more reasonable on the log scale (this is considered Model 2).
```{r}
lm_model2 <- lm(log(count) ~ ., data = training_df)
summary(lm_model2)
```

```{r}
par(mfrow = c(2,2))
plot(lm_model2, which = c(1,2,3))
```

With this new model, the residuals vs fitted plot no longer increased as the values increased, and the qq-plot also shows that normality is a reasonable assumption. Noticing that ‘holiday’ and ‘day’ were not statistically significant in this model based on the t-tests, we created another model based on the log transformed model where ‘holiday’ and ‘day’ were taken out (this model is considered Model 3). 
```{r}
lm_model3 <- lm(log(count) ~ .-holiday -day, data = training_df) # no holiday and day

```

```{r}
par(mfrow = c(2,2))
plot(lm_model3, which = c(1,2,3))
```

An ANOVA was then run on this model with the following hypotheses: 

    Null Hypothesis: Model 3 is a reasonable fit
    
    Alternative Hypothesis: Model 2 is a reasonable fit
```{r}
anova(lm_model3,lm_model2)
```

The p-value is greater than 0.05, so we do not reject the null hypothesis, which means Model 3 is adequate to model the rentals. There's no evidence that including ‘holiday’ and ‘day’ improves the model's fit.

An ANOVA was also run on Model 3 to confirm that all the variables included have statistical significance even after adjustment for the other variables. If any of the variables were not statistically significant, we would have continued our model selection by removing those variables. However, all the variables were statistically significant, so we continued our analysis with Model 3.

```{r}
summary(lm_model3)
anova(lm_model3)
```

# Predicting Holidays using Logit
To learn more about what factors are useful in predicting if a day is a holiday in the dataset, a logit model was used since holiday is a categorical variable and has options 0 for not a holiday and 1 for is a holiday. First the data needed to be cleaned a bit more since we planned to use the function bestglm. Bestglm tests all possible combinations of factors and finds the one with the best AIC score so the more factors put in the longer the run time. To minimize the run time and also not run the code mindlessly, factors ‘holiday’, ‘atemp’, ‘year’, ‘month’, ‘hour’, and ‘dayofweek’ were removed due to them not being relevant. The factor ‘workingday’ was also converted to a categorical variable. 
```{r}
logclean = training_df
logclean$y = logclean[,2] # renaming holiday to y
logclean = logclean[,-2] # removing holiday
logclean = logclean[,-5] # removing atemp
logclean = logclean[,-8] # removing year
logclean = logclean[,-9] # removing month
logclean = logclean[,-9] # removing hour
logclean = logclean[,-9] # removing dayofweek

#convert columns into factors
logclean$workingday = factor(logclean$workingday)
logclean$y <- ifelse(logclean$y == 1,TRUE,FALSE)
logclean$y = factor(logclean$y)

str(logclean)
```

After running bestglm on the dataset, the best model had an AIC of 2082, with only variables `season`, `windspeed` and `workingday`.  Although this AIC is quite high, this is the model that has the best fit for predicting if it's a holiday.
```{r}

log.bestglm <- bestglm(Xy = logclean, family = binomial,
            IC = "AIC",                
            method = "exhaustive")
summary(log.bestglm)
log.bestglm$BestModels
summary(log.bestglm$BestModels)
```

# Using Tree based models to predict temperature using important features
We are training decision tree and random forest to predict the temperature.

Decision tree is a predictive model that visually represents a series of decisions based on features of input data.It consists of nodes that represent decision points, branches that correspond to possible outcomes of those decisions and leaves that represent final predictions or classifications.The tree is built by recursively partitioning the data based on the values of different features until a stopping criterion is met.

Random forest is an ensemble learning method that consists of a collection  of decision trees.Each tree in the forest is trained on a random subset of the training data and during prediction, the results from individual trees are combined.This combination can involve voting(for classification)  and averaging(for regression).The randomness introduced in the training process helps to reduce overfitting and improve generalization.

While training the decision tree we hard coded ‘min_split’ which determines the minimum number of observations that must exist in the node for a split to be attempted during construction of the decision tree. We can tune this parameter accordingly. We used train-test-split(80-20) to train and evaluate the models.If we analyze the summary of the decision tree, as we increase the number of splits, the relative error decreases.
```{r warning=FALSE}

library(ggplot2)
library(ISLR)
p1 <- ggplot(training_df,aes(temp, count, color = season))
print(p1 + geom_point(size = 4))

```

```{r warning=FALSE}

table(training_df$workingday)

copied_df = data.frame(training_df)



library(rpart)
library(rpart.plot)
library(randomForest)




copied_df = copied_df[, -c(1,4)]

train_percentage <- 0.8

# Generate random indices for the training set
indices <- sample(1:nrow(copied_df), size = round(train_percentage * nrow(copied_df)), replace = FALSE)

# Create the training set
train_set <- copied_df[indices, ]

# Create the testing set
test_set <- copied_df[-indices, ]


train_features <- train_set[, -which(names(train_set) == "temp")]
train_target <- train_set$temp

# Separate features and target in the testing set
test_features <- test_set[, -which(names(test_set) == "temp")]
test_target <- test_set$temp


tree <- rpart(train_target ~ .,method = 'anova', data = train_features,minsplit = 5)

predictions <- predict(tree, newdata = test_features)

rmse <- sqrt(mean((test_target - predictions)^2))

printcp(tree)
```

Another important point which we noticed was there is a noticeable difference between the variance of features using in model training.So to solve this problem and to generate better and robust predictions we trained a bagging model because it has a built in regularization to minimize overfitting  and can handle datasets with varying feature importance.If we check the feature vs node purity graph, ‘humidity’, ‘atemp’, and ‘windspeed’ seem to be the most important features with highest impurities.
```{r warning=FALSE}
print(paste("humidity:", var(train_features$humidity)))
print(paste("atemp:", var(train_features$atemp)))
print(paste("windspeed:", var(train_features$windspeed)))
```

While evaluating the models we chose RMSE which is root mean squared error, which is a very common metric used in regression problems, because it is the same units as the target variable making it highly interpretable. It gives a clear idea of  average magnitude of errors in the original scale of the target variable; moreover it is sensitive to outliers which can be fruitful if we want the model to penalize more for larger discrepancies.
```{r warning=FALSE}
# Display the RMSE
print(paste("Decision Tree Root Mean Squared Error:", rmse))

rf.model <- randomForest(train_target ~ ., method = 'anova', data = train_features)


predictions_rfm <- predict(rf.model, newdata = test_features)

rmse_rfm <- sqrt(mean((test_target - predictions_rfm)^2))

print(paste("Random Forest Root Mean Squared Error for rfm:", rmse_rfm))

```

# Results
After going through all the processes in our pipeline we are able to answer our 3 SMART questions. In the first question we need to extract the features which are useful for predicting the number of bikes used in an hourly granularity. After doing model selection based on the linear regression models, we came to the conclusion that the variables ‘holiday’ and ‘day’ are not statistically significant variables, unlike the other variables. Model 3 proved to be the most adequate model for the rental of bikes per hour due to the ANOVA we ran to compare Model 3 and Model 2. The additional ANOVA ran on Model 3 also proved that removing any more of the variables would reduce model fit. This means that all the variables other than ‘holiday’ and ‘day’ are significant and important for predicting the number of bike rentals in an hour. The final list of variables that are useful for predicting the number of bikes used in an hour are ‘season’, ‘working day’, ‘weather’, ‘temp’, ‘atemp’, ‘humidity’, ‘windspeed’, ‘year’, ‘month’, ‘hour’, and ‘dayofweek’.
```{r}
summary(lm_model3)
```

In the second SMART question we need to check the relationship between temperature (independent variable) and the rental counts. According to our EDA plot, the bike count increases with higher temperature but drops off once temperatures crossed 30 degrees Celsius. This makes sense considering 20-30 degrees Celsius is considered the most comfortable temperature range to be outside. The number of rental counts also increased with increasing humidity. 
```{r, echo=FALSE}
ggplot(train_data, aes(temp, count)) + geom_point(alpha = 0.3) + theme_bw() +
  labs(title = "Number of Bikers by Temperature") +
  xlab("Temperature (Celcius)") +
  ylab("Number of Bikers")
```

```{r, echo=FALSE}

ggplot(train_data, aes(humidity, count)) + geom_point(alpha = 0.3) + theme_bw() +
  labs(title = "Number of Bikers by Humidity") +
  xlab("Humidity (%)") +
  ylab("Number of Bikers")

```

Our final model (Model 3) also shows a slight increase in the number of rental counts when temperature increases due to the positive coefficient in the table. 
```{r, echo=FALSE}
summary(lm_model3)$coefficients["temp","Estimate"]
```

In the third SMART question we need to answer when do the casual (or non-registered) bikers increase the most. To answer this question, we plotted a scatterplot between temperature and the number of casual bike users. The number of casual bike rentals increased when the temperature increased but dropped off once the temperature started going beyond 30 degrees Celsius. Most of them rented bikes when the temperature was between 20-30 degrees Celsius. In Fahrenheit that is between 68-86 degrees. Again, this makes sense as people tend to go outside only when the temperature is nice and not too hot or cold.
```{r casual bikers, echo= 'FALSE'}
ggplot(train_data, aes(casual, temp))+ geom_point() + theme_light() +
  labs(x = "Number of Casual Bike Users", 
       y = "Temperature (celcius)",
       title = "Casual Bikers by Temperature")
```

In the 4th SMART question, we tried to estimate when it’s a holiday depending on our list of variables. After running a logit model using bestglm, the best model is discovered to only be influenced by `season`, `windspeed` and `workingday`, with an AIC of 2082. This was the model with the lowest AIC and can be considered to be the best fit for predicting a holiday day. 
```{r bestglm}
summary(log.bestglm$BestModels)
```

To answer SMART question 5, the best model was found based on running random forest and decision tree models to estimate the temperature. Based on the RMSE results below, we found that temperature is best predicted by using the features ‘humidity’, ‘windspeed’, and ‘datetime' based on the random forest model.
```{r warning=FALSE}
# Display the RMSE
print(paste("Decision Tree Root Mean Squared Error:", rmse))

print(paste("Random Forest Root Mean Squared Error for rfm:", rmse_rfm))

```

# Conclusions
In conclusion, through the process of this analysis, we were able to explore the bike sharing data provided and practiced EDA while also learning how to create models and to help answer our SMART questions. As a potential next step for future consideration, we will use the model we selected to generate predictions on a test dataset by predicting the number of bike rentals per hour. We will also try to use some more advanced techniques like hyperparameter tuning and model pruning if we get high error scores. 

## Recommendation
 
Sustainability Goals for increasing Bike usage in different Cities 
Urban Municipal cities' commitments to sustainability and reducing congestion and pollution can drive the adoption of bike sharing as a sustainable transportation solution.
Sustainability Bike sharing and usage have significance and can vary from one city or region to another. Successful bike-sharing programs often consider these factors when planning and implementing their services.


# References
 
Black, C., and S. Potter. Undated. Portsmouth bikeabout: A smart-card bike club scheme. http://www.metrobike.net/index.php?s=file_download&id=11/.

DeMaio, P. 2003. Smart bikes: Public transportation for the 21st century. Transpor tation Quarterly 57(1): 9–11.

DeMaio, P. 2004. Will smart bikes succeed as public transportation in the United States? Journal of Public Transportation 7(2): 1-15. 

DeMaio, P. 2007. What a year for bike-sharing. December 30. http://bike-sharing.
blogspot.com/2007/12/what-year-for-bike-sharing.html/. 

DeMaio, Paul. 2008a. Bike-sharing Wrap-up for 2008. December 31. http://bike sharing.blogspot.com/2008/12/bike-sharing-wrap-up-for-2008.html/.

DeMaio, P. 2008b. University bike-sharing’s time has come. The Bike-sharing Blog. October 26. http://bike-sharing.blogspot.com/2008/10/university-bike-shar ings-time-has-come.html/.

MetroBike. 2009. The Bike-sharing world map. http://maps.google.com/maps/ ms?ie=UTF8&hl=en&om=1&msa=0&msid=104227318304000014160.00043
d80f9456b3416ced&ll=43.580391,-42.890625&spn=143.80149,154.6875&z=1 &source=embed/

Nielse, B. H. 1993. The Bicycle in Denmark: Present Use and Future Potential. Danish Ministry of Transport. 52

Optimising Bike Sharing in European Cities. 2009a. France. FR_Rennes_Old System tab. http://www.obisproject.com/palio/html.run?_Instance=obis&_PageID=4&_LngID=21&_CatID=722&pic=4&_CheckSum=1012982978/. 

Optimising Bike Sharing in European Cities. 2009b. Germany. DE_Berlin tab.http://www.obisproject.com/palio/html.run?_Instance=obis&_PageID=4&_ LngID=21&_CatID=722&pic=4&_CheckSum=1012982978/. 

Optimising Bike Sharing in European Cities. 2009c. France. FR_Lyon tab. http:// www.obisproject.com/palio/html.run?_Instance=obis&_PageID=4&_ LngID=21&_CatID=722&pic=4&_CheckSum=1012982978/
Schimmelpennick, L. 2009. Conversation with author. March 5

Cukierski, W. (2014). Bike Sharing Demand. Kaggle. https://www.kaggle.com/competitions/bike-sharing-demand/overview